{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carga de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/corpus.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>helpful</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>category</th>\n",
       "      <th>processedReview</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A36BI9SPO3LTC8</td>\n",
       "      <td>B0023ZQDEC</td>\n",
       "      <td>Gerbera Daisy</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>Not a good fit for me. Much too big. I didn't ...</td>\n",
       "      <td>3</td>\n",
       "      <td>Not good for me</td>\n",
       "      <td>1360972800</td>\n",
       "      <td>02 16, 2013</td>\n",
       "      <td>0</td>\n",
       "      <td>clothing</td>\n",
       "      <td>good fit much enough bust fit back may work so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AL1WTXQJCKT8X</td>\n",
       "      <td>B00HWF7KPY</td>\n",
       "      <td>Granny \"Joyous-soul\"</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>good fit- not to thin- love the less thicker  ...</td>\n",
       "      <td>5</td>\n",
       "      <td>Surprised</td>\n",
       "      <td>1403308800</td>\n",
       "      <td>06 21, 2014</td>\n",
       "      <td>0</td>\n",
       "      <td>clothing</td>\n",
       "      <td>good love little thick weist band many read ta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A2QOT4L1QFYYIW</td>\n",
       "      <td>B003VFGW76</td>\n",
       "      <td>lu shenglan</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>Quality is very goodMy husband liked itThe nex...</td>\n",
       "      <td>5</td>\n",
       "      <td>Quality is very good</td>\n",
       "      <td>1387152000</td>\n",
       "      <td>12 16, 2013</td>\n",
       "      <td>0</td>\n",
       "      <td>clothing</td>\n",
       "      <td>quality goodmy husband like itthe next time bu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A2KQUKZ4X13BI0</td>\n",
       "      <td>B001CGW432</td>\n",
       "      <td>musica_al</td>\n",
       "      <td>[2, 2]</td>\n",
       "      <td>I like wearing bungee style slip on shoes.  Th...</td>\n",
       "      <td>5</td>\n",
       "      <td>Just what I wanted</td>\n",
       "      <td>1305331200</td>\n",
       "      <td>05 14, 2011</td>\n",
       "      <td>0</td>\n",
       "      <td>clothing</td>\n",
       "      <td>like wear bungee style slip can limit shoe wan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ATJOVP7P6JRPU</td>\n",
       "      <td>B005OBWI3W</td>\n",
       "      <td>T.W. Connell</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>I thought the bracelet would be packaged in a ...</td>\n",
       "      <td>3</td>\n",
       "      <td>Nice product poor packaging</td>\n",
       "      <td>1355356800</td>\n",
       "      <td>12 13, 2012</td>\n",
       "      <td>0</td>\n",
       "      <td>clothing</td>\n",
       "      <td>think bracelet package simple box arrive packa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       reviewerID        asin          reviewerName helpful  \\\n",
       "0  A36BI9SPO3LTC8  B0023ZQDEC         Gerbera Daisy  [0, 0]   \n",
       "1   AL1WTXQJCKT8X  B00HWF7KPY  Granny \"Joyous-soul\"  [0, 0]   \n",
       "2  A2QOT4L1QFYYIW  B003VFGW76           lu shenglan  [0, 0]   \n",
       "3  A2KQUKZ4X13BI0  B001CGW432             musica_al  [2, 2]   \n",
       "4   ATJOVP7P6JRPU  B005OBWI3W          T.W. Connell  [0, 0]   \n",
       "\n",
       "                                          reviewText  overall  \\\n",
       "0  Not a good fit for me. Much too big. I didn't ...        3   \n",
       "1  good fit- not to thin- love the less thicker  ...        5   \n",
       "2  Quality is very goodMy husband liked itThe nex...        5   \n",
       "3  I like wearing bungee style slip on shoes.  Th...        5   \n",
       "4  I thought the bracelet would be packaged in a ...        3   \n",
       "\n",
       "                       summary  unixReviewTime   reviewTime  sentiment  \\\n",
       "0              Not good for me      1360972800  02 16, 2013          0   \n",
       "1                    Surprised      1403308800  06 21, 2014          0   \n",
       "2         Quality is very good      1387152000  12 16, 2013          0   \n",
       "3           Just what I wanted      1305331200  05 14, 2011          0   \n",
       "4  Nice product poor packaging      1355356800  12 13, 2012          0   \n",
       "\n",
       "   category                                    processedReview  \n",
       "0  clothing  good fit much enough bust fit back may work so...  \n",
       "1  clothing  good love little thick weist band many read ta...  \n",
       "2  clothing  quality goodmy husband like itthe next time bu...  \n",
       "3  clothing  like wear bungee style slip can limit shoe wan...  \n",
       "4  clothing  think bracelet package simple box arrive packa...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conjuntos de train y test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos los conjuntos de entrenamiento (75% del total) y test (25%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df['processedReview'],\n",
    "    df['sentiment'],\n",
    "    train_size=0.75,\n",
    "    test_size=0.25,\n",
    "    random_state=42,\n",
    "    shuffle=True,\n",
    "    stratify=df['sentiment']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La extracción de características será realizada con la clase `TfidfVectorizer`de `sklearn`. Los parámetros y valores que se definen son:\n",
    "- **ngram_range**: (1, 3), de cara a codificar de manera más rica el contexto de cada palabra se trabajará con 1-grams, 2-grams y 3-grams.\n",
    "- **max_df**: 0.98, al haber realizado una etapa de preprocesado en la que se eliminan stopwords y se lemmatiza no se requiere volver a eliminar tokens muy frecuentes.\n",
    "- **min_df**: 3, tokens con una frecuencia baja se interpretan como outliers, posiblemente typos u otro tipo de errores.\n",
    "- **max_features**: 10000, al trabajar con 1-gram, 2-grams y 3-grams seleccionamos un tamaño de vocabulario grande que nos permita contemplar los top n-grams más representativos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cv = TfidfVectorizer(\n",
    "    ngram_range=(1, 3),\n",
    "    max_df=0.98,\n",
    "    min_df=3,\n",
    "    max_features=10000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv.fit(X_train)\n",
    "\n",
    "X_train_ = cv.transform(X_train)\n",
    "X_test_ = cv.transform(X_test)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para este apartado empelaremos dos modelos que son adecuados para tareas de clasificación binaria con variables discretas. La aproximación será simple en el sentido de que no se destinarán esfuerzos al tuning de hiperparametros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regresión Logística"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model = LogisticRegression(C=1, solver='lbfgs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_model.fit(X_train_, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_train_predict = lr_model.predict(X_train_)\n",
    "lr_test_predict = lr_model.predict(X_test_)\n",
    "lr_test_predict_proba = lr_model.predict_proba(X_test_)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(lr_model, open('../models/lr_model.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb_model = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb_model.fit(X_train_, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculamos las predicciones para test, train y las probabilidades para test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb_train_predict = mnb_model.predict(X_train_)\n",
    "mnb_test_predict = mnb_model.predict(X_test_)\n",
    "mnb_test_predict_proba = mnb_model.predict_proba(X_test_)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(mnb_model, open('../models/mnb_model.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preds = pd.DataFrame({\n",
    "    'y_actual': y_test,\n",
    "    'y_pred_test_lr': lr_test_predict,\n",
    "    'y_pred_proba_test_lr': lr_test_predict_proba,\n",
    "    'y_pred_test_mnb': mnb_test_predict,\n",
    "    'y_pred_proba_test_mnb': mnb_test_predict_proba\n",
    "})\n",
    "\n",
    "df_preds.to_csv('../results/preds.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_model_metrics(y_test, y_test_predict):\n",
    "    print('Confussion matrix:\\n{}'.format(confusion_matrix(y_test, y_test_predict)))\n",
    "    print('\\nClassification report:\\n{}'.format(classification_report(y_test, y_test_predict)))\n",
    "    print('Accuracy score:{}'.format(accuracy_score(y_test, y_test_predict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confussion matrix:\n",
      "[[1463  410]\n",
      " [ 364 1510]]\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.78      0.79      1873\n",
      "           1       0.79      0.81      0.80      1874\n",
      "\n",
      "    accuracy                           0.79      3747\n",
      "   macro avg       0.79      0.79      0.79      3747\n",
      "weighted avg       0.79      0.79      0.79      3747\n",
      "\n",
      "Accuracy score:0.7934347477982386\n"
     ]
    }
   ],
   "source": [
    "print_model_metrics(y_test, lr_test_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confussion matrix:\n",
      "[[1465  408]\n",
      " [ 392 1482]]\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.78      0.79      1873\n",
      "           1       0.78      0.79      0.79      1874\n",
      "\n",
      "    accuracy                           0.79      3747\n",
      "   macro avg       0.79      0.79      0.79      3747\n",
      "weighted avg       0.79      0.79      0.79      3747\n",
      "\n",
      "Accuracy score:0.7864958633573526\n"
     ]
    }
   ],
   "source": [
    "print_model_metrics(y_test, mnb_test_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusiones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tras entrenar dos modelos sencillos y calcular algunas métricas para los conjuntos de test podemos determinar:\n",
    "- El tiempo de cómputo no es problema para ninguno de los dos modelos.\n",
    "- Ambos modelos devuelven métricas similares.\n",
    "- Sin apenas trabajo en perfeccionar esta etapa (tuning de hiperparámetros, cross validation, variación en los parámetros en la etapa de extracción de características...) los resultados permiten afirmar que el problema es abordable.\n",
    "- El mejor modelo, aunque no por mucho, es la Regresión Logística, ya que todas sus métricas son superiores y su tasa de falsos positivos (muy interesante e importante en este tipo de problemas) es menor.\n",
    "- Sería interesante analizar el performance de modelos de boosting o basados en árboles.\n",
    "\n",
    "Por tanto, el modelo seleccionado es la Regresión Logística."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
